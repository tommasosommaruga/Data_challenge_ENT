{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial Graph NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Neural Networks (GNNs) are a type of deep learning model specifically designed to work with data that is structured as a graph, where entities are represented by nodes and relationships are captured by edges. Unlike traditional neural networks, GNNs are able to account for the connections between nodes, learning from how they interact with one another. These models have proven highly effective in tasks like predicting links between nodes, classifying nodes, and even analyzing entire graphs, making them useful in fields like social networks, drug discovery, and recommendation systems.\n",
    "\n",
    "This is why we are trying this technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Import already preprocessed dataset \n",
    "from `data_splitting_1` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "train_data = pd.read_pickle('Data/train_dataset_title_sim.pkl')\n",
    "test_data = pd.read_pickle('Data/test_dataset_title_sim.pkl')\n",
    "\n",
    "X_train_raw = train_data.drop(columns=['title', 'title_b', 'abstract', 'abstract_b', 'citations', 'citations_b', 'index', 'index_b', 'label'])\n",
    "y_train = train_data['label']\n",
    "X_test_raw = test_data.drop(columns=['title', 'title_b', 'abstract', 'abstract_b', 'citations', 'citations_b', 'index', 'index_b', 'label'])\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_a', 'paper_b', 'year', 'venue', 'category_0', 'category_1',\n",
       "       'category_2', 'category_3', 'category_4', 'category_5', 'category_6',\n",
       "       'category_7', 'category_8', 'category_9', 'category_10', 'category_11',\n",
       "       'category_12', 'category_13', 'category_14', 'category_15',\n",
       "       'category_16', 'category_17', 'category_18', 'category_19',\n",
       "       'category_20', 'category_21', 'category_22', 'category_23',\n",
       "       'category_24', 'category_25', 'category_26', 'category_27',\n",
       "       'category_28', 'category_29', 'category_30', 'category_31',\n",
       "       'category_32', 'category_33', 'category_34', 'year_b', 'venue_b',\n",
       "       'category_0_b', 'category_1_b', 'category_2_b', 'category_3_b',\n",
       "       'category_4_b', 'category_5_b', 'category_6_b', 'category_7_b',\n",
       "       'category_8_b', 'category_9_b', 'category_10_b', 'category_11_b',\n",
       "       'category_12_b', 'category_13_b', 'category_14_b', 'category_15_b',\n",
       "       'category_16_b', 'category_17_b', 'category_18_b', 'category_19_b',\n",
       "       'category_20_b', 'category_21_b', 'category_22_b', 'category_23_b',\n",
       "       'category_24_b', 'category_25_b', 'category_26_b', 'category_27_b',\n",
       "       'category_28_b', 'category_29_b', 'category_30_b', 'category_31_b',\n",
       "       'category_32_b', 'category_33_b', 'category_34_b', 'title_similarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a Graph Convolutional Network (GCN) that leverages graph-structured data to learn node representations through message-passing, using two graph convolution layers followed by a sigmoid output for binary classification. Despite its ability to capture relationships between nodes, the model performs suboptimally with an accuracy around 0.7305, indicating room for improvement in both architecture and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Step 1: Define the function to create edge indices\n",
    "def create_edge_index(data, num_nodes):\n",
    "    # Create a mapping from unique paper IDs to numeric IDs\n",
    "    paper_ids = pd.concat([data['paper_a'], data['paper_b']]).unique()\n",
    "    paper_id_map = {paper: idx for idx, paper in enumerate(paper_ids)}\n",
    "\n",
    "    edge_index = []\n",
    "    for _, row in data.iterrows():\n",
    "        if row['label'] == 1:  # Add an edge if there's a citation\n",
    "            paper_a_id = paper_id_map.get(row['paper_a'])\n",
    "            paper_b_id = paper_id_map.get(row['paper_b'])\n",
    "            if paper_a_id is not None and paper_b_id is not None:\n",
    "                # Ensure indices are within bounds\n",
    "                if paper_a_id < num_nodes and paper_b_id < num_nodes:\n",
    "                    edge_index.append([paper_a_id, paper_b_id])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Validate that all indices are within bounds\n",
    "    if edge_index.numel() > 0 and edge_index.max().item() >= num_nodes:\n",
    "        raise ValueError(\"Edge index contains out-of-bound indices!\")\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare edge indices\n",
    "num_nodes_train = X_train.shape[0]\n",
    "num_nodes_test = X_test.shape[0]\n",
    "\n",
    "edge_index_train = create_edge_index(train_data, num_nodes=num_nodes_train)\n",
    "edge_index_test = create_edge_index(test_data, num_nodes=num_nodes_test)\n",
    "\n",
    "# Step 3: Prepare PyTorch Geometric Data Objects\n",
    "data_train = Data(\n",
    "    x=torch.tensor(X_train, dtype=torch.float),\n",
    "    edge_index=edge_index_train,\n",
    "    y=torch.tensor(y_train.values, dtype=torch.float)\n",
    ")\n",
    "\n",
    "data_test = Data(\n",
    "    x=torch.tensor(X_test, dtype=torch.float),\n",
    "    edge_index=edge_index_test,\n",
    "    y=torch.tensor(y_test.values, dtype=torch.float)\n",
    ")\n",
    "data_train = data_train.to(device)\n",
    "data_test = data_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8140\n",
      "Epoch 10, Loss: 0.6453\n",
      "Epoch 20, Loss: 0.6095\n",
      "Epoch 30, Loss: 0.5901\n",
      "Epoch 40, Loss: 0.5791\n",
      "Epoch 50, Loss: 0.5706\n",
      "Epoch 60, Loss: 0.5643\n",
      "Epoch 70, Loss: 0.5591\n",
      "Epoch 80, Loss: 0.5544\n",
      "Epoch 90, Loss: 0.5505\n",
      "Epoch 100, Loss: 0.5475\n",
      "Epoch 110, Loss: 0.5451\n",
      "Epoch 120, Loss: 0.5433\n",
      "Epoch 130, Loss: 0.5418\n",
      "Epoch 140, Loss: 0.5407\n",
      "Epoch 150, Loss: 0.5394\n",
      "Epoch 160, Loss: 0.5384\n",
      "Epoch 170, Loss: 0.5374\n",
      "Epoch 180, Loss: 0.5365\n",
      "Epoch 190, Loss: 0.5357\n",
      "Epoch 200, Loss: 0.5350\n",
      "Epoch 210, Loss: 0.5343\n",
      "Epoch 220, Loss: 0.5337\n",
      "Epoch 230, Loss: 0.5331\n",
      "Epoch 240, Loss: 0.5325\n",
      "Epoch 250, Loss: 0.5320\n",
      "Epoch 260, Loss: 0.5314\n",
      "Epoch 270, Loss: 0.5308\n",
      "Epoch 280, Loss: 0.5301\n",
      "Epoch 290, Loss: 0.5293\n",
      "Epoch 300, Loss: 0.5283\n",
      "Epoch 310, Loss: 0.5271\n",
      "Epoch 320, Loss: 0.5261\n",
      "Epoch 330, Loss: 0.5253\n",
      "Epoch 340, Loss: 0.5246\n",
      "Epoch 350, Loss: 0.5240\n",
      "Epoch 360, Loss: 0.5236\n",
      "Epoch 370, Loss: 0.5232\n",
      "Epoch 380, Loss: 0.5228\n",
      "Epoch 390, Loss: 0.5225\n",
      "Epoch 400, Loss: 0.5222\n",
      "Epoch 410, Loss: 0.5219\n",
      "Epoch 420, Loss: 0.5216\n",
      "Epoch 430, Loss: 0.5214\n",
      "Epoch 440, Loss: 0.5211\n",
      "Epoch 450, Loss: 0.5209\n",
      "Epoch 460, Loss: 0.5207\n",
      "Epoch 470, Loss: 0.5205\n",
      "Epoch 480, Loss: 0.5202\n",
      "Epoch 490, Loss: 0.5200\n",
      "Epoch 500, Loss: 0.5200\n",
      "Epoch 510, Loss: 0.5196\n",
      "Epoch 520, Loss: 0.5195\n",
      "Epoch 530, Loss: 0.5193\n",
      "Epoch 540, Loss: 0.5192\n",
      "Epoch 550, Loss: 0.5192\n",
      "Epoch 560, Loss: 0.5190\n",
      "Epoch 570, Loss: 0.5188\n",
      "Epoch 580, Loss: 0.5187\n",
      "Epoch 590, Loss: 0.5186\n",
      "Epoch 600, Loss: 0.5185\n",
      "Epoch 610, Loss: 0.5184\n",
      "Epoch 620, Loss: 0.5183\n",
      "Epoch 630, Loss: 0.5181\n",
      "Epoch 640, Loss: 0.5181\n",
      "Epoch 650, Loss: 0.5179\n",
      "Epoch 660, Loss: 0.5178\n",
      "Epoch 670, Loss: 0.5177\n",
      "Epoch 680, Loss: 0.5177\n",
      "Epoch 690, Loss: 0.5177\n",
      "Epoch 700, Loss: 0.5175\n",
      "Epoch 710, Loss: 0.5174\n",
      "Epoch 720, Loss: 0.5173\n",
      "Epoch 730, Loss: 0.5172\n",
      "Epoch 740, Loss: 0.5172\n",
      "Epoch 750, Loss: 0.5171\n",
      "Epoch 760, Loss: 0.5170\n",
      "Epoch 770, Loss: 0.5171\n",
      "Epoch 780, Loss: 0.5169\n",
      "Epoch 790, Loss: 0.5168\n",
      "Epoch 800, Loss: 0.5167\n",
      "Epoch 810, Loss: 0.5166\n",
      "Epoch 820, Loss: 0.5167\n",
      "Epoch 830, Loss: 0.5165\n",
      "Epoch 840, Loss: 0.5164\n",
      "Epoch 850, Loss: 0.5164\n",
      "Epoch 860, Loss: 0.5163\n",
      "Epoch 870, Loss: 0.5163\n",
      "Epoch 880, Loss: 0.5162\n",
      "Epoch 890, Loss: 0.5161\n",
      "Epoch 900, Loss: 0.5161\n",
      "Epoch 910, Loss: 0.5161\n",
      "Epoch 920, Loss: 0.5159\n",
      "Epoch 930, Loss: 0.5158\n",
      "Epoch 940, Loss: 0.5157\n",
      "Epoch 950, Loss: 0.5156\n",
      "Epoch 960, Loss: 0.5156\n",
      "Epoch 970, Loss: 0.5155\n",
      "Epoch 980, Loss: 0.5157\n",
      "Epoch 990, Loss: 0.5154\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 1)  # Binary output\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.sigmoid(x).squeeze()  # Binary probability output\n",
    "\n",
    "# Step 5: Train the GCN Model\n",
    "def train_model(model, data, epochs=1000, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "# Step 6: Initialize and train GCN\n",
    "gcn = GCN(num_node_features=X_train.shape[1])\n",
    "gcn = gcn.to(device)\n",
    "gcn = train_model(gcn, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GCN:\n",
      "Accuracy: 0.7267\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate GCN Model\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        predictions = (out > 0.5).float()  # Convert probabilities to binary predictions\n",
    "        accuracy = (predictions == data.y).sum().item() / len(data.y)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        return predictions\n",
    "\n",
    "print(\"\\nEvaluating GCN:\")\n",
    "gcn_predictions = evaluate_model(gcn, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## improved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying model with an additional convolutional layer, applies dropout after each convolutional layer to prevent overfitting, and incorporates early stopping during training to halt the process when the loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ImprovedGCN(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, hidden_units=64, dropout_rate=0.5):\n",
    "        super(ImprovedGCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(num_node_features, hidden_units)\n",
    "        self.conv2 = GCNConv(hidden_units, hidden_units)\n",
    "        self.conv3 = GCNConv(hidden_units, num_classes)  # Third layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        return torch.sigmoid(x)  # Use sigmoid for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, epochs=100, lr=0.01, patience=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data).squeeze()  # Squeeze to match the shape of the labels\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter > patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data).squeeze()  # Squeeze to match the labels shape\n",
    "        pred = (out > 0.5).float()  # Convert to binary predictions (0 or 1)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = (pred == data.y).sum().item() / len(data.y)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return out, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8820\n",
      "Epoch 10, Loss: 0.6748\n",
      "Epoch 20, Loss: 0.6379\n",
      "Epoch 30, Loss: 0.6158\n",
      "Epoch 40, Loss: 0.6014\n",
      "Epoch 50, Loss: 0.5921\n",
      "Epoch 60, Loss: 0.5857\n",
      "Epoch 70, Loss: 0.5791\n",
      "Epoch 80, Loss: 0.5727\n",
      "Epoch 90, Loss: 0.5684\n",
      "Epoch 100, Loss: 0.5644\n",
      "Epoch 110, Loss: 0.5610\n",
      "Epoch 120, Loss: 0.5580\n",
      "Epoch 130, Loss: 0.5558\n",
      "Epoch 140, Loss: 0.5540\n",
      "Epoch 150, Loss: 0.5531\n",
      "Epoch 160, Loss: 0.5505\n",
      "Epoch 170, Loss: 0.5502\n",
      "Epoch 180, Loss: 0.5498\n",
      "Epoch 190, Loss: 0.5486\n",
      "Epoch 200, Loss: 0.5473\n",
      "Epoch 210, Loss: 0.5476\n",
      "Epoch 220, Loss: 0.5466\n",
      "Epoch 230, Loss: 0.5461\n",
      "Epoch 240, Loss: 0.5459\n",
      "Epoch 250, Loss: 0.5454\n",
      "Early stopping at epoch 254\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the Improved GCN\n",
    "gcn = ImprovedGCN(num_node_features=X_train.shape[1], num_classes=1)\n",
    "gcn = gcn.to(device)\n",
    "gcn = train_model(gcn, data_train, epochs=1000, lr=0.005, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GCN:\n",
      "Accuracy: 0.7295\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating GCN:\")\n",
    "gcn_predictions = evaluate_model(gcn, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really improved\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying new method\n",
    "See if improves if the model is more compplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Load Data\n",
    "train_data = pd.read_pickle('Data/train_dataset_title_sim.pkl')\n",
    "test_data = pd.read_pickle('Data/test_dataset_title_sim.pkl')\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Step 2: Preprocess Features\n",
    "def preprocess_data(df):\n",
    "    features = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith('category'):\n",
    "            features.append(df[col].values)  # Category features\n",
    "\n",
    "    # Add additional features\n",
    "    features.append(df['year'].values)\n",
    "    features.append(df['venue'].values)\n",
    "    features.append(df['year_b'].values)\n",
    "    features.append(df['venue_b'].values)\n",
    "    features.append(df['title_similarity'].values)\n",
    "\n",
    "    # Convert to 2D array\n",
    "    features = np.column_stack(features)\n",
    "    return features\n",
    "\n",
    "X_train = preprocess_data(train_data)\n",
    "X_test = preprocess_data(test_data)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Create Edge Indices\n",
    "def create_edge_index(data, num_nodes):\n",
    "    paper_ids = pd.concat([data['paper_a'], data['paper_b']]).unique()\n",
    "    paper_id_map = {paper: idx for idx, paper in enumerate(paper_ids)}\n",
    "\n",
    "    edge_index = []\n",
    "    for _, row in data.iterrows():\n",
    "        if row['label'] == 1:  # Add an edge for citation\n",
    "            paper_a_id = paper_id_map.get(row['paper_a'])\n",
    "            paper_b_id = paper_id_map.get(row['paper_b'])\n",
    "            if paper_a_id is not None and paper_b_id is not None:\n",
    "                if paper_a_id < num_nodes and paper_b_id < num_nodes:\n",
    "                    edge_index.append([paper_a_id, paper_b_id])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Validate edge indices\n",
    "    if edge_index.numel() > 0 and edge_index.max().item() >= num_nodes:\n",
    "        raise ValueError(\"Edge index contains out-of-bound indices!\")\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "num_nodes_train = X_train.shape[0]\n",
    "num_nodes_test = X_test.shape[0]\n",
    "\n",
    "edge_index_train = create_edge_index(train_data, num_nodes=num_nodes_train)\n",
    "edge_index_test = create_edge_index(test_data, num_nodes=num_nodes_test)\n",
    "\n",
    "# Step 4: Create PyTorch Geometric Data Objects\n",
    "data_train = Data(\n",
    "    x=torch.tensor(X_train, dtype=torch.float),\n",
    "    edge_index=edge_index_train,\n",
    "    y=torch.tensor(y_train.values, dtype=torch.float)\n",
    ")\n",
    "\n",
    "data_test = Data(\n",
    "    x=torch.tensor(X_test, dtype=torch.float),\n",
    "    edge_index=edge_index_test,\n",
    "    y=torch.tensor(y_test.values, dtype=torch.float)\n",
    ")\n",
    "data_train = data_train.to(device)\n",
    "data_test = data_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7476\n",
      "Epoch 10, Loss: 0.7014\n",
      "Epoch 20, Loss: 0.6429\n",
      "Epoch 30, Loss: 0.6044\n",
      "Epoch 40, Loss: 0.5781\n",
      "Epoch 50, Loss: 0.5615\n",
      "Epoch 60, Loss: 0.5491\n",
      "Epoch 70, Loss: 0.5431\n",
      "Epoch 80, Loss: 0.5360\n",
      "Epoch 90, Loss: 0.5302\n",
      "Epoch 100, Loss: 0.5282\n",
      "Epoch 110, Loss: 0.5703\n",
      "Early stopping at epoch 117\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Step 1: Define a more complex GCN model with dense layers added\n",
    "class ComplexGCNWithDense(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, hidden_units=128, dropout_rate=0.5):\n",
    "        super(ComplexGCNWithDense, self).__init__()\n",
    "\n",
    "        # Layer 1: First Convolutional Layer\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_units)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_units)  # Batch Normalization\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 2: Second Convolutional Layer\n",
    "        self.conv2 = GCNConv(hidden_units, hidden_units * 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_units * 2)  # Batch Normalization\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 3: Third Convolutional Layer\n",
    "        self.conv3 = GCNConv(hidden_units * 2, hidden_units * 2)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_units * 2)  # Batch Normalization\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 4: Fourth Convolutional Layer\n",
    "        self.conv4 = GCNConv(hidden_units * 2, hidden_units * 2)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_units * 2)  # Batch Normalization\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 5: Final Convolutional Layer (to reduce to single output node)\n",
    "        self.conv5 = GCNConv(hidden_units * 2, num_classes)\n",
    "\n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(num_classes, hidden_units * 2)  # Dense layer after GCN layers\n",
    "        self.bn_fc1 = nn.BatchNorm1d(hidden_units * 2)  # Batch Normalization for FC layer\n",
    "        self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_units * 2, hidden_units)  # Dense layer\n",
    "        self.bn_fc2 = nn.BatchNorm1d(hidden_units)  # Batch Normalization for FC layer\n",
    "        self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_units, num_classes)  # Output layer for binary classification\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First Layer (GCN + Dropout + BatchNorm)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second Layer (GCN + Dropout + BatchNorm)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Third Layer (GCN + Dropout + BatchNorm)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Fourth Layer (GCN + Dropout + BatchNorm)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Fifth Layer: Output (GCN)\n",
    "        x = self.conv5(x, edge_index)\n",
    "        \n",
    "        # Flatten and pass through fully connected (dense) layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.dropout_fc1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn_fc2(x)\n",
    "        x = self.dropout_fc2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x).squeeze()  # Binary output for classification\n",
    "\n",
    "# Step 2: Train the Model (same as before)\n",
    "def train_model(model, data, epochs=1000, lr=0.01, patience=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data).squeeze()  # Squeeze to match the shape of the labels\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter > patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Step 3: Initialize and Train the Model\n",
    "gcn = ComplexGCNWithDense(num_node_features=X_train.shape[1], num_classes=1, hidden_units=128, dropout_rate=0.5)\n",
    "gcn = gcn.to(device)\n",
    "gcn = train_model(gcn, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7174\n",
      "Test Precision: 0.6898\n",
      "Test Recall: 0.7900\n",
      "Test F1 Score: 0.7365\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "def evaluate_model(model, data, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    data = data.to(device)  # Move the data to the appropriate device (GPU or CPU)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get predictions\n",
    "        out = model(data)\n",
    "        predictions = (out >= 0.5).float()  # Apply threshold\n",
    "\n",
    "        # Convert tensors to lists for sklearn metrics\n",
    "        y_true = data.y.detach().cpu().tolist()\n",
    "        y_pred = predictions.detach().cpu().tolist()\n",
    "\n",
    "        # Compute metrics using sklearn\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Ensure test data is on the correct device\n",
    "data_test = data_test.to(device)\n",
    "\n",
    "# Perform evaluation on test data\n",
    "accuracy, precision, recall, f1 = evaluate_model(gcn, data_test, device)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained seems to not be better than the normal models, probably these models need to be tuned better and they may be too simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
