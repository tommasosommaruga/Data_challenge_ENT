{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial Graph NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "train_data = pd.read_pickle('Data/train_dataset_title_sim.pkl')\n",
    "test_data = pd.read_pickle('Data/test_dataset_title_sim.pkl')\n",
    "\n",
    "X_train_raw = train_data.drop(columns=['title', 'title_b', 'abstract', 'abstract_b', 'citations', 'citations_b', 'index', 'index_b', 'label'])\n",
    "y_train = train_data['label']\n",
    "X_test_raw = test_data.drop(columns=['title', 'title_b', 'abstract', 'abstract_b', 'citations', 'citations_b', 'index', 'index_b', 'label'])\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_a', 'paper_b', 'year', 'venue', 'category_0', 'category_1',\n",
       "       'category_2', 'category_3', 'category_4', 'category_5', 'category_6',\n",
       "       'category_7', 'category_8', 'category_9', 'category_10', 'category_11',\n",
       "       'category_12', 'category_13', 'category_14', 'category_15',\n",
       "       'category_16', 'category_17', 'category_18', 'category_19',\n",
       "       'category_20', 'category_21', 'category_22', 'category_23',\n",
       "       'category_24', 'category_25', 'category_26', 'category_27',\n",
       "       'category_28', 'category_29', 'category_30', 'category_31',\n",
       "       'category_32', 'category_33', 'category_34', 'year_b', 'venue_b',\n",
       "       'category_0_b', 'category_1_b', 'category_2_b', 'category_3_b',\n",
       "       'category_4_b', 'category_5_b', 'category_6_b', 'category_7_b',\n",
       "       'category_8_b', 'category_9_b', 'category_10_b', 'category_11_b',\n",
       "       'category_12_b', 'category_13_b', 'category_14_b', 'category_15_b',\n",
       "       'category_16_b', 'category_17_b', 'category_18_b', 'category_19_b',\n",
       "       'category_20_b', 'category_21_b', 'category_22_b', 'category_23_b',\n",
       "       'category_24_b', 'category_25_b', 'category_26_b', 'category_27_b',\n",
       "       'category_28_b', 'category_29_b', 'category_30_b', 'category_31_b',\n",
       "       'category_32_b', 'category_33_b', 'category_34_b', 'title_similarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7682\n",
      "Epoch 10, Loss: 0.6239\n",
      "Epoch 20, Loss: 0.5951\n",
      "Epoch 30, Loss: 0.5823\n",
      "Epoch 40, Loss: 0.5743\n",
      "Epoch 50, Loss: 0.5672\n",
      "Epoch 60, Loss: 0.5608\n",
      "Epoch 70, Loss: 0.5551\n",
      "Epoch 80, Loss: 0.5503\n",
      "Epoch 90, Loss: 0.5468\n",
      "Epoch 100, Loss: 0.5446\n",
      "Epoch 110, Loss: 0.5429\n",
      "Epoch 120, Loss: 0.5416\n",
      "Epoch 130, Loss: 0.5405\n",
      "Epoch 140, Loss: 0.5396\n",
      "Epoch 150, Loss: 0.5387\n",
      "Epoch 160, Loss: 0.5380\n",
      "Epoch 170, Loss: 0.5373\n",
      "Epoch 180, Loss: 0.5366\n",
      "Epoch 190, Loss: 0.5360\n",
      "Epoch 200, Loss: 0.5355\n",
      "Epoch 210, Loss: 0.5351\n",
      "Epoch 220, Loss: 0.5347\n",
      "Epoch 230, Loss: 0.5342\n",
      "Epoch 240, Loss: 0.5339\n",
      "Epoch 250, Loss: 0.5335\n",
      "Epoch 260, Loss: 0.5331\n",
      "Epoch 270, Loss: 0.5327\n",
      "Epoch 280, Loss: 0.5323\n",
      "Epoch 290, Loss: 0.5319\n",
      "Epoch 300, Loss: 0.5315\n",
      "Epoch 310, Loss: 0.5312\n",
      "Epoch 320, Loss: 0.5309\n",
      "Epoch 330, Loss: 0.5305\n",
      "Epoch 340, Loss: 0.5302\n",
      "Epoch 350, Loss: 0.5299\n",
      "Epoch 360, Loss: 0.5297\n",
      "Epoch 370, Loss: 0.5294\n",
      "Epoch 380, Loss: 0.5292\n",
      "Epoch 390, Loss: 0.5289\n",
      "Epoch 400, Loss: 0.5287\n",
      "Epoch 410, Loss: 0.5284\n",
      "Epoch 420, Loss: 0.5281\n",
      "Epoch 430, Loss: 0.5278\n",
      "Epoch 440, Loss: 0.5274\n",
      "Epoch 450, Loss: 0.5271\n",
      "Epoch 460, Loss: 0.5268\n",
      "Epoch 470, Loss: 0.5266\n",
      "Epoch 480, Loss: 0.5264\n",
      "Epoch 490, Loss: 0.5261\n",
      "Epoch 500, Loss: 0.5258\n",
      "Epoch 510, Loss: 0.5255\n",
      "Epoch 520, Loss: 0.5252\n",
      "Epoch 530, Loss: 0.5249\n",
      "Epoch 540, Loss: 0.5246\n",
      "Epoch 550, Loss: 0.5242\n",
      "Epoch 560, Loss: 0.5238\n",
      "Epoch 570, Loss: 0.5234\n",
      "Epoch 580, Loss: 0.5230\n",
      "Epoch 590, Loss: 0.5227\n",
      "Epoch 600, Loss: 0.5223\n",
      "Epoch 610, Loss: 0.5221\n",
      "Epoch 620, Loss: 0.5217\n",
      "Epoch 630, Loss: 0.5214\n",
      "Epoch 640, Loss: 0.5215\n",
      "Epoch 650, Loss: 0.5210\n",
      "Epoch 660, Loss: 0.5208\n",
      "Epoch 670, Loss: 0.5207\n",
      "Epoch 680, Loss: 0.5205\n",
      "Epoch 690, Loss: 0.5204\n",
      "Epoch 700, Loss: 0.5203\n",
      "Epoch 710, Loss: 0.5202\n",
      "Epoch 720, Loss: 0.5201\n",
      "Epoch 730, Loss: 0.5200\n",
      "Epoch 740, Loss: 0.5199\n",
      "Epoch 750, Loss: 0.5198\n",
      "Epoch 760, Loss: 0.5197\n",
      "Epoch 770, Loss: 0.5196\n",
      "Epoch 780, Loss: 0.5198\n",
      "Epoch 790, Loss: 0.5194\n",
      "Epoch 800, Loss: 0.5193\n",
      "Epoch 810, Loss: 0.5192\n",
      "Epoch 820, Loss: 0.5191\n",
      "Epoch 830, Loss: 0.5191\n",
      "Epoch 840, Loss: 0.5190\n",
      "Epoch 850, Loss: 0.5189\n",
      "Epoch 860, Loss: 0.5188\n",
      "Epoch 870, Loss: 0.5187\n",
      "Epoch 880, Loss: 0.5187\n",
      "Epoch 890, Loss: 0.5186\n",
      "Epoch 900, Loss: 0.5185\n",
      "Epoch 910, Loss: 0.5184\n",
      "Epoch 920, Loss: 0.5184\n",
      "Epoch 930, Loss: 0.5183\n",
      "Epoch 940, Loss: 0.5183\n",
      "Epoch 950, Loss: 0.5182\n",
      "Epoch 960, Loss: 0.5181\n",
      "Epoch 970, Loss: 0.5181\n",
      "Epoch 980, Loss: 0.5182\n",
      "Epoch 990, Loss: 0.5180\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Step 1: Define the function to create edge indices\n",
    "def create_edge_index(data, num_nodes):\n",
    "    # Create a mapping from unique paper IDs to numeric IDs\n",
    "    paper_ids = pd.concat([data['paper_a'], data['paper_b']]).unique()\n",
    "    paper_id_map = {paper: idx for idx, paper in enumerate(paper_ids)}\n",
    "\n",
    "    edge_index = []\n",
    "    for _, row in data.iterrows():\n",
    "        if row['label'] == 1:  # Add an edge if there's a citation\n",
    "            paper_a_id = paper_id_map.get(row['paper_a'])\n",
    "            paper_b_id = paper_id_map.get(row['paper_b'])\n",
    "            if paper_a_id is not None and paper_b_id is not None:\n",
    "                # Ensure indices are within bounds\n",
    "                if paper_a_id < num_nodes and paper_b_id < num_nodes:\n",
    "                    edge_index.append([paper_a_id, paper_b_id])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Validate that all indices are within bounds\n",
    "    if edge_index.numel() > 0 and edge_index.max().item() >= num_nodes:\n",
    "        raise ValueError(\"Edge index contains out-of-bound indices!\")\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Step 2: Prepare edge indices\n",
    "num_nodes_train = X_train.shape[0]\n",
    "num_nodes_test = X_test.shape[0]\n",
    "\n",
    "edge_index_train = create_edge_index(train_data, num_nodes=num_nodes_train)\n",
    "edge_index_test = create_edge_index(test_data, num_nodes=num_nodes_test)\n",
    "\n",
    "# Step 3: Prepare PyTorch Geometric Data Objects\n",
    "data_train = Data(\n",
    "    x=torch.tensor(X_train, dtype=torch.float),\n",
    "    edge_index=edge_index_train,\n",
    "    y=torch.tensor(y_train.values, dtype=torch.float)\n",
    ")\n",
    "\n",
    "data_test = Data(\n",
    "    x=torch.tensor(X_test, dtype=torch.float),\n",
    "    edge_index=edge_index_test,\n",
    "    y=torch.tensor(y_test.values, dtype=torch.float)\n",
    ")\n",
    "\n",
    "# Step 4: Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 1)  # Binary output\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.sigmoid(x).squeeze()  # Binary probability output\n",
    "\n",
    "# Step 5: Train the GCN Model\n",
    "def train_model(model, data, epochs=1000, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "# Step 6: Initialize and train GCN\n",
    "gcn = GCN(num_node_features=X_train.shape[1])\n",
    "gcn = train_model(gcn, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GCN:\n",
      "Accuracy: 0.7277\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate GCN Model\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        predictions = (out > 0.5).float()  # Convert probabilities to binary predictions\n",
    "        accuracy = (predictions == data.y).sum().item() / len(data.y)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        return predictions\n",
    "\n",
    "print(\"\\nEvaluating GCN:\")\n",
    "gcn_predictions = evaluate_model(gcn, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ImprovedGCN(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, hidden_units=64, dropout_rate=0.5):\n",
    "        super(ImprovedGCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(num_node_features, hidden_units)\n",
    "        self.conv2 = GCNConv(hidden_units, hidden_units)\n",
    "        self.conv3 = GCNConv(hidden_units, num_classes)  # Third layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        return torch.sigmoid(x)  # Use sigmoid for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, epochs=100, lr=0.01, patience=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data).squeeze()  # Squeeze to match the shape of the labels\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter > patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data).squeeze()  # Squeeze to match the labels shape\n",
    "        pred = (out > 0.5).float()  # Convert to binary predictions (0 or 1)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = (pred == data.y).sum().item() / len(data.y)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return out, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Improved GCN\n",
    "#gcn = ImprovedGCN(num_node_features=X_train.shape[1], num_classes=1)\n",
    "#gcn = train_model(gcn, data_train, epochs=1000, lr=0.005, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GCN:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating GCN:\")\n",
    "#gcn_predictions = evaluate_model(gcn, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "train_data = pd.read_pickle('Data/train_dataset_title_sim.pkl')\n",
    "test_data = pd.read_pickle('Data/test_dataset_title_sim.pkl')\n",
    "\n",
    "# Extracting the label column\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['paper_a', 'paper_b', 'label', 'title', 'year', 'venue', 'index',\n",
       "        'citations', 'abstract', 'category_0', 'category_1', 'category_2',\n",
       "        'category_3', 'category_4', 'category_5', 'category_6', 'category_7',\n",
       "        'category_8', 'category_9', 'category_10', 'category_11', 'category_12',\n",
       "        'category_13', 'category_14', 'category_15', 'category_16',\n",
       "        'category_17', 'category_18', 'category_19', 'category_20',\n",
       "        'category_21', 'category_22', 'category_23', 'category_24',\n",
       "        'category_25', 'category_26', 'category_27', 'category_28',\n",
       "        'category_29', 'category_30', 'category_31', 'category_32',\n",
       "        'category_33', 'category_34', 'title_b', 'year_b', 'venue_b', 'index_b',\n",
       "        'citations_b', 'abstract_b', 'category_0_b', 'category_1_b',\n",
       "        'category_2_b', 'category_3_b', 'category_4_b', 'category_5_b',\n",
       "        'category_6_b', 'category_7_b', 'category_8_b', 'category_9_b',\n",
       "        'category_10_b', 'category_11_b', 'category_12_b', 'category_13_b',\n",
       "        'category_14_b', 'category_15_b', 'category_16_b', 'category_17_b',\n",
       "        'category_18_b', 'category_19_b', 'category_20_b', 'category_21_b',\n",
       "        'category_22_b', 'category_23_b', 'category_24_b', 'category_25_b',\n",
       "        'category_26_b', 'category_27_b', 'category_28_b', 'category_29_b',\n",
       "        'category_30_b', 'category_31_b', 'category_32_b', 'category_33_b',\n",
       "        'category_34_b', 'title_similarity'],\n",
       "       dtype='object'),\n",
       " Index(['paper_a', 'paper_b', 'label', 'title', 'year', 'venue', 'index',\n",
       "        'citations', 'abstract', 'category_0', 'category_1', 'category_2',\n",
       "        'category_3', 'category_4', 'category_5', 'category_6', 'category_7',\n",
       "        'category_8', 'category_9', 'category_10', 'category_11', 'category_12',\n",
       "        'category_13', 'category_14', 'category_15', 'category_16',\n",
       "        'category_17', 'category_18', 'category_19', 'category_20',\n",
       "        'category_21', 'category_22', 'category_23', 'category_24',\n",
       "        'category_25', 'category_26', 'category_27', 'category_28',\n",
       "        'category_29', 'category_30', 'category_31', 'category_32',\n",
       "        'category_33', 'category_34', 'title_b', 'year_b', 'venue_b', 'index_b',\n",
       "        'citations_b', 'abstract_b', 'category_0_b', 'category_1_b',\n",
       "        'category_2_b', 'category_3_b', 'category_4_b', 'category_5_b',\n",
       "        'category_6_b', 'category_7_b', 'category_8_b', 'category_9_b',\n",
       "        'category_10_b', 'category_11_b', 'category_12_b', 'category_13_b',\n",
       "        'category_14_b', 'category_15_b', 'category_16_b', 'category_17_b',\n",
       "        'category_18_b', 'category_19_b', 'category_20_b', 'category_21_b',\n",
       "        'category_22_b', 'category_23_b', 'category_24_b', 'category_25_b',\n",
       "        'category_26_b', 'category_27_b', 'category_28_b', 'category_29_b',\n",
       "        'category_30_b', 'category_31_b', 'category_32_b', 'category_33_b',\n",
       "        'category_34_b', 'title_similarity'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns, test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9715\n",
      "Epoch 10, Loss: 0.6665\n",
      "Epoch 20, Loss: 0.6163\n",
      "Epoch 30, Loss: 0.5951\n",
      "Epoch 40, Loss: 0.5834\n",
      "Epoch 50, Loss: 0.5752\n",
      "Epoch 60, Loss: 0.5692\n",
      "Epoch 70, Loss: 0.5646\n",
      "Epoch 80, Loss: 0.5608\n",
      "Epoch 90, Loss: 0.5574\n",
      "Epoch 100, Loss: 0.5547\n",
      "Epoch 110, Loss: 0.5522\n",
      "Epoch 120, Loss: 0.5502\n",
      "Epoch 130, Loss: 0.5486\n",
      "Epoch 140, Loss: 0.5473\n",
      "Epoch 150, Loss: 0.5462\n",
      "Epoch 160, Loss: 0.5452\n",
      "Epoch 170, Loss: 0.5443\n",
      "Epoch 180, Loss: 0.5435\n",
      "Epoch 190, Loss: 0.5428\n",
      "Epoch 200, Loss: 0.5421\n",
      "Epoch 210, Loss: 0.5415\n",
      "Epoch 220, Loss: 0.5409\n",
      "Epoch 230, Loss: 0.5403\n",
      "Epoch 240, Loss: 0.5397\n",
      "Epoch 250, Loss: 0.5392\n",
      "Epoch 260, Loss: 0.5387\n",
      "Epoch 270, Loss: 0.5382\n",
      "Epoch 280, Loss: 0.5378\n",
      "Epoch 290, Loss: 0.5374\n",
      "Epoch 300, Loss: 0.5371\n",
      "Epoch 310, Loss: 0.5367\n",
      "Epoch 320, Loss: 0.5364\n",
      "Epoch 330, Loss: 0.5361\n",
      "Epoch 340, Loss: 0.5359\n",
      "Epoch 350, Loss: 0.5356\n",
      "Epoch 360, Loss: 0.5354\n",
      "Epoch 370, Loss: 0.5351\n",
      "Epoch 380, Loss: 0.5349\n",
      "Epoch 390, Loss: 0.5347\n",
      "Epoch 400, Loss: 0.5345\n",
      "Epoch 410, Loss: 0.5343\n",
      "Epoch 420, Loss: 0.5340\n",
      "Epoch 430, Loss: 0.5337\n",
      "Epoch 440, Loss: 0.5334\n",
      "Epoch 450, Loss: 0.5330\n",
      "Epoch 460, Loss: 0.5327\n",
      "Epoch 470, Loss: 0.5324\n",
      "Epoch 480, Loss: 0.5320\n",
      "Epoch 490, Loss: 0.5316\n",
      "Epoch 500, Loss: 0.5312\n",
      "Epoch 510, Loss: 0.5308\n",
      "Epoch 520, Loss: 0.5302\n",
      "Epoch 530, Loss: 0.5297\n",
      "Epoch 540, Loss: 0.5292\n",
      "Epoch 550, Loss: 0.5288\n",
      "Epoch 560, Loss: 0.5283\n",
      "Epoch 570, Loss: 0.5279\n",
      "Epoch 580, Loss: 0.5275\n",
      "Epoch 590, Loss: 0.5271\n",
      "Epoch 600, Loss: 0.5268\n",
      "Epoch 610, Loss: 0.5264\n",
      "Epoch 620, Loss: 0.5260\n",
      "Epoch 630, Loss: 0.5257\n",
      "Epoch 640, Loss: 0.5254\n",
      "Epoch 650, Loss: 0.5252\n",
      "Epoch 660, Loss: 0.5250\n",
      "Epoch 670, Loss: 0.5249\n",
      "Epoch 680, Loss: 0.5247\n",
      "Epoch 690, Loss: 0.5246\n",
      "Epoch 700, Loss: 0.5244\n",
      "Epoch 710, Loss: 0.5243\n",
      "Epoch 720, Loss: 0.5242\n",
      "Epoch 730, Loss: 0.5240\n",
      "Epoch 740, Loss: 0.5239\n",
      "Epoch 750, Loss: 0.5238\n",
      "Epoch 760, Loss: 0.5237\n",
      "Epoch 770, Loss: 0.5236\n",
      "Epoch 780, Loss: 0.5235\n",
      "Epoch 790, Loss: 0.5234\n",
      "Epoch 800, Loss: 0.5233\n",
      "Epoch 810, Loss: 0.5231\n",
      "Epoch 820, Loss: 0.5231\n",
      "Epoch 830, Loss: 0.5230\n",
      "Epoch 840, Loss: 0.5229\n",
      "Epoch 850, Loss: 0.5228\n",
      "Epoch 860, Loss: 0.5228\n",
      "Epoch 870, Loss: 0.5227\n",
      "Epoch 880, Loss: 0.5226\n",
      "Epoch 890, Loss: 0.5225\n",
      "Epoch 900, Loss: 0.5225\n",
      "Epoch 910, Loss: 0.5224\n",
      "Epoch 920, Loss: 0.5223\n",
      "Epoch 930, Loss: 0.5222\n",
      "Epoch 940, Loss: 0.5221\n",
      "Epoch 950, Loss: 0.5221\n",
      "Epoch 960, Loss: 0.5220\n",
      "Epoch 970, Loss: 0.5220\n",
      "Epoch 980, Loss: 0.5219\n",
      "Epoch 990, Loss: 0.5219\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Load Data\n",
    "train_data = pd.read_pickle('Data/train_dataset_title_sim.pkl')\n",
    "test_data = pd.read_pickle('Data/test_dataset_title_sim.pkl')\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Step 2: Preprocess Features\n",
    "def preprocess_data(df):\n",
    "    features = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith('category'):\n",
    "            features.append(df[col].values)  # Category features\n",
    "\n",
    "    # Add additional features\n",
    "    features.append(df['year'].values)\n",
    "    features.append(df['venue'].values)\n",
    "    features.append(df['year_b'].values)\n",
    "    features.append(df['venue_b'].values)\n",
    "    features.append(df['title_similarity'].values)\n",
    "\n",
    "    # Convert to 2D array\n",
    "    features = np.column_stack(features)\n",
    "    return features\n",
    "\n",
    "X_train = preprocess_data(train_data)\n",
    "X_test = preprocess_data(test_data)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Create Edge Indices\n",
    "def create_edge_index(data, num_nodes):\n",
    "    paper_ids = pd.concat([data['paper_a'], data['paper_b']]).unique()\n",
    "    paper_id_map = {paper: idx for idx, paper in enumerate(paper_ids)}\n",
    "\n",
    "    edge_index = []\n",
    "    for _, row in data.iterrows():\n",
    "        if row['label'] == 1:  # Add an edge for citation\n",
    "            paper_a_id = paper_id_map.get(row['paper_a'])\n",
    "            paper_b_id = paper_id_map.get(row['paper_b'])\n",
    "            if paper_a_id is not None and paper_b_id is not None:\n",
    "                if paper_a_id < num_nodes and paper_b_id < num_nodes:\n",
    "                    edge_index.append([paper_a_id, paper_b_id])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Validate edge indices\n",
    "    if edge_index.numel() > 0 and edge_index.max().item() >= num_nodes:\n",
    "        raise ValueError(\"Edge index contains out-of-bound indices!\")\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "num_nodes_train = X_train.shape[0]\n",
    "num_nodes_test = X_test.shape[0]\n",
    "\n",
    "edge_index_train = create_edge_index(train_data, num_nodes=num_nodes_train)\n",
    "edge_index_test = create_edge_index(test_data, num_nodes=num_nodes_test)\n",
    "\n",
    "# Step 4: Create PyTorch Geometric Data Objects\n",
    "data_train = Data(\n",
    "    x=torch.tensor(X_train, dtype=torch.float),\n",
    "    edge_index=edge_index_train,\n",
    "    y=torch.tensor(y_train.values, dtype=torch.float)\n",
    ")\n",
    "\n",
    "data_test = Data(\n",
    "    x=torch.tensor(X_test, dtype=torch.float),\n",
    "    edge_index=edge_index_test,\n",
    "    y=torch.tensor(y_test.values, dtype=torch.float)\n",
    ")\n",
    "\n",
    "# Step 5: Define GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 1)  # Binary output\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.sigmoid(x).squeeze()  # Binary probability output\n",
    "\n",
    "# Step 6: Train GCN Model\n",
    "def train_model(model, data, epochs=1000, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "# Step 7: Initialize and Train the Model\n",
    "gcn = GCN(num_node_features=X_train.shape[1])\n",
    "gcn = train_model(gcn, data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7257\n",
      "Test Precision: 0.7198\n",
      "Test Recall: 0.7392\n",
      "Test F1 Score: 0.7294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get predictions\n",
    "        out = model(data)\n",
    "        predictions = (out >= 0.5).float()  # Apply threshold\n",
    "\n",
    "        # Compute metrics\n",
    "        y_true = data.y.cpu().numpy()\n",
    "        y_pred = predictions.cpu().numpy()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Perform evaluation on test data\n",
    "accuracy, precision, recall, f1 = evaluate_model(gcn, data_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained seems to not be better than the normal models, probably these models need to be tuned better and they may be too simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
