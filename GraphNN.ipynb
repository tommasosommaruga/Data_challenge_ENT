{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial Graph NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Neural Networks (GNNs) are a type of deep learning model specifically designed to work with data that is structured as a graph, where entities are represented by nodes and relationships are captured by edges. Unlike traditional neural networks, GNNs are able to account for the connections between nodes, learning from how they interact with one another. These models have proven highly effective in tasks like predicting links between nodes, classifying nodes, and even analyzing entire graphs, making them useful in fields like social networks, drug discovery, and recommendation systems.\n",
    "\n",
    "This is why we are trying this technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Import already preprocessed dataset \n",
    "from `data_splitting_1` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('Data\\\\preprocessed_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>index</th>\n",
       "      <th>citations</th>\n",
       "      <th>abstract</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>...</th>\n",
       "      <th>category_25</th>\n",
       "      <th>category_26</th>\n",
       "      <th>category_27</th>\n",
       "      <th>category_28</th>\n",
       "      <th>category_29</th>\n",
       "      <th>category_30</th>\n",
       "      <th>category_31</th>\n",
       "      <th>category_32</th>\n",
       "      <th>category_33</th>\n",
       "      <th>category_34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57929</th>\n",
       "      <td>Proceedings of the 15th International Conferen...</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>57929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3640</th>\n",
       "      <td>Digital Reference</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>3640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447127</th>\n",
       "      <td>Freenet P</td>\n",
       "      <td>1900</td>\n",
       "      <td>1</td>\n",
       "      <td>447127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192455</th>\n",
       "      <td>A linear algebraic theory of complexes</td>\n",
       "      <td>1941</td>\n",
       "      <td>1</td>\n",
       "      <td>192455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185904</th>\n",
       "      <td>The embedding of products and joins of complex...</td>\n",
       "      <td>1947</td>\n",
       "      <td>1</td>\n",
       "      <td>185904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  year  venue  \\\n",
       "57929   Proceedings of the 15th International Conferen...  2008      1   \n",
       "3640                                    Digital Reference  2006      1   \n",
       "447127                                          Freenet P  1900      1   \n",
       "192455             A linear algebraic theory of complexes  1941      1   \n",
       "185904  The embedding of products and joins of complex...  1947      1   \n",
       "\n",
       "         index citations abstract  category_0  category_1  category_2  \\\n",
       "57929    57929       NaN      NaN           0           0           0   \n",
       "3640      3640       NaN      NaN           0           0           0   \n",
       "447127  447127       NaN      NaN           0           0           0   \n",
       "192455  192455       NaN      NaN           0           0           0   \n",
       "185904  185904       NaN      NaN           0           0           0   \n",
       "\n",
       "        category_3  ...  category_25  category_26  category_27  category_28  \\\n",
       "57929            0  ...            0            0            0            0   \n",
       "3640             0  ...            0            0            0            0   \n",
       "447127           0  ...            0            0            0            0   \n",
       "192455           0  ...            0            0            0            0   \n",
       "185904           0  ...            0            0            0            0   \n",
       "\n",
       "        category_29  category_30  category_31  category_32  category_33  \\\n",
       "57929             0            0            0            0            0   \n",
       "3640              0            0            0            0            0   \n",
       "447127            0            0            0            0            0   \n",
       "192455            0            0            0            0            0   \n",
       "185904            0            0            0            0            0   \n",
       "\n",
       "        category_34  \n",
       "57929             1  \n",
       "3640              0  \n",
       "447127            1  \n",
       "192455            0  \n",
       "185904            1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 503845 papers, 100297 citations\n",
      "Testing set: 125962 papers, 25075 citations\n",
      "Training pairs: 100000 (Positive: 50000, Negative: 50000)\n",
      "Testing pairs: 20000 (Positive: 10000, Negative: 10000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "df.fillna('', inplace=True)\n",
    "# Split the data by paper indices, ensuring balanced class distribution (stratify)\n",
    "def split_data_balanced(df, test_size=0.2):\n",
    "    # Create a label column for stratification (0 = no citations, 1 = citations)\n",
    "    df['label'] = df['citations'].apply(lambda x: 1 if pd.notna(x) and x != '' else 0)\n",
    "\n",
    "    # Split the dataset while maintaining the class distribution (stratify by 'label')\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, stratify=df['label'], random_state=55)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Optimized function to create positive pairs\n",
    "def create_positive_pairs(df, max_positive):\n",
    "    pairs = []\n",
    "    indices_set = set(df['index'].values)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if len(pairs) >= max_positive:\n",
    "            break\n",
    "        citing_paper = row['index']\n",
    "        cited_papers = row['citations'].split(';') if row['citations'] else []\n",
    "        valid_cited_papers = [cited for cited in cited_papers if cited in indices_set]\n",
    "        \n",
    "        for cited in valid_cited_papers:\n",
    "            if len(pairs) >= max_positive:\n",
    "                break\n",
    "            pairs.append((citing_paper, cited, 1))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "# Optimized function to create negative pairs\n",
    "def create_negative_pairs(df, max_negative, num_negatives=2):\n",
    "    pairs = []\n",
    "    indices = df['index'].tolist()\n",
    "    indices_set = set(indices)\n",
    "    \n",
    "    while len(pairs) < max_negative:\n",
    "        citing_paper = random.choice(indices)\n",
    "        negative_samples = random.sample(indices, num_negatives)\n",
    "        \n",
    "        for neg in negative_samples:\n",
    "            if len(pairs) >= max_negative:\n",
    "                break\n",
    "            if neg != citing_paper and neg in indices_set:\n",
    "                pairs.append((citing_paper, neg, 0))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Parameters\n",
    "max_pairs_train = 100000\n",
    "max_positive_train = max_pairs_train // 2\n",
    "max_negative_train = max_pairs_train - max_positive_train\n",
    "\n",
    "max_pairs_test = 20000\n",
    "max_positive_test = max_pairs_test // 2\n",
    "max_negative_test = max_pairs_test - max_positive_test\n",
    "# Example dataset split with balanced classes in train and test sets\n",
    "train_df, test_df = split_data_balanced(df, test_size=0.2)\n",
    "print(f\"Training set: {len(train_df)} papers, {train_df['label'].sum()} citations\")\n",
    "print(f\"Testing set: {len(test_df)} papers, {test_df['label'].sum()} citations\")\n",
    "# Generate positive and negative pairs for training, ensuring only training papers are used\n",
    "train_positive_pairs = create_positive_pairs(train_df, max_positive_train)\n",
    "train_negative_pairs = create_negative_pairs(train_df, max_negative_train, num_negatives=2)\n",
    "train_pairs = train_positive_pairs + train_negative_pairs\n",
    "random.shuffle(train_pairs)\n",
    "\n",
    "# Generate positive and negative pairs for testing, ensuring only test papers are used\n",
    "test_positive_pairs = create_positive_pairs(test_df, max_positive_test)\n",
    "test_negative_pairs = create_negative_pairs(test_df, max_negative_test, num_negatives=2)\n",
    "test_pairs = test_positive_pairs + test_negative_pairs\n",
    "random.shuffle(test_pairs)\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_pairs_df = pd.DataFrame(train_pairs, columns=['paper_a', 'paper_b', 'label'])\n",
    "test_pairs_df = pd.DataFrame(test_pairs, columns=['paper_a', 'paper_b', 'label'])\n",
    "\n",
    "# Check distribution\n",
    "print(f\"Training pairs: {len(train_pairs_df)} (Positive: {len([p for p in train_pairs if p[2] == 1])}, Negative: {len([p for p in train_pairs if p[2] == 0])})\")\n",
    "print(f\"Testing pairs: {len(test_pairs_df)} (Positive: {len([p for p in test_pairs if p[2] == 1])}, Negative: {len([p for p in test_pairs if p[2] == 0])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'year', 'venue', 'index', 'citations', 'abstract',\n",
       "       'category_0', 'category_1', 'category_2', 'category_3', 'category_4',\n",
       "       'category_5', 'category_6', 'category_7', 'category_8', 'category_9',\n",
       "       'category_10', 'category_11', 'category_12', 'category_13',\n",
       "       'category_14', 'category_15', 'category_16', 'category_17',\n",
       "       'category_18', 'category_19', 'category_20', 'category_21',\n",
       "       'category_22', 'category_23', 'category_24', 'category_25',\n",
       "       'category_26', 'category_27', 'category_28', 'category_29',\n",
       "       'category_30', 'category_31', 'category_32', 'category_33',\n",
       "       'category_34', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Function to add venue features\n",
    "def add_venue_features(merged_ab):\n",
    "    # Count occurrences of each venue in paper_a and paper_b\n",
    "    venue_counts = Counter(merged_ab['venue'].fillna('') + merged_ab['venue_b'].fillna(''))\n",
    "\n",
    "    # Compute common venue and count encoding features\n",
    "    merged_ab['common_venue'] = (merged_ab['venue'] == merged_ab['venue_b']).astype(int)\n",
    "    merged_ab['venue_a_count'] = merged_ab['venue'].map(venue_counts).fillna(0)\n",
    "    merged_ab['venue_b_count'] = merged_ab['venue_b'].map(venue_counts).fillna(0)\n",
    "\n",
    "    # Drop original venue columns\n",
    "    merged_ab.drop(columns=['venue', 'venue_b'], inplace=True)\n",
    "    \n",
    "    return merged_ab\n",
    "\n",
    "def prepare_features(pairs_df, df, fit_tfidf=False, tfidf_vectorizer=None):\n",
    "    # Merge pairs_df with df to get features for paper_a\n",
    "    merged_a = pairs_df.merge(df, left_on='paper_a', right_on='index', suffixes=('', '_a'))\n",
    "    merged_ab = merged_a.merge(df, left_on='paper_b', right_on='index', suffixes=('', '_b'))\n",
    "\n",
    "    # Combine the titles for comparison\n",
    "    titles_a = merged_ab['title'].fillna('')  # Handle missing titles in paper_a\n",
    "    titles_b = merged_ab['title_b'].fillna('')  # Handle missing titles in paper_b\n",
    "\n",
    "    if fit_tfidf:\n",
    "        # If fit_tfidf is True, fit the TF-IDF vectorizer on the titles of the training set\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_vectorizer.fit(titles_a)\n",
    "    \n",
    "    # Transform titles_a and titles_b using the same fitted tfidf_vectorizer\n",
    "    tfidf_matrix_a = tfidf_vectorizer.transform(titles_a)\n",
    "    tfidf_matrix_b = tfidf_vectorizer.transform(titles_b)\n",
    "\n",
    "    # Compute pairwise cosine similarity for aligned pairs\n",
    "    title_similarity = [cosine_similarity(tfidf_matrix_a[i], tfidf_matrix_b[i])[0][0] for i in range(tfidf_matrix_a.shape[0])]\n",
    "\n",
    "    # Assign title similarity scores to the merged DataFrame\n",
    "    merged_ab['title_similarity'] = title_similarity\n",
    "\n",
    "    # Select features for model (same as in the original implementation)\n",
    "    features = merged_ab.drop(columns=['citations', 'citations_b', 'index', 'index_b', 'label', 'title', 'title_b', 'abstract', 'abstract_b'])\n",
    "\n",
    "    # Add venue and author features\n",
    "    features = add_venue_features(features)\n",
    "\n",
    "    # Extract labels if they exist in the pairs DataFrame\n",
    "    labels = merged_ab['label'] if 'label' in merged_ab.columns else None\n",
    "\n",
    "    return features, labels, tfidf_vectorizer, merged_ab\n",
    "\n",
    "# Example usage for train set:\n",
    "train_features, train_labels, tfidf_vectorizer, merged_ab_train = prepare_features(train_pairs_df, df, fit_tfidf=True)\n",
    "\n",
    "# Example usage for test set:\n",
    "test_features, test_labels, _, merged_ab_test = prepare_features(test_pairs_df, df, fit_tfidf=False, tfidf_vectorizer=tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_features.copy(), train_labels.copy(), test_features.copy(), test_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_a</th>\n",
       "      <th>paper_b</th>\n",
       "      <th>year</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>category_6</th>\n",
       "      <th>...</th>\n",
       "      <th>category_30_b</th>\n",
       "      <th>category_31_b</th>\n",
       "      <th>category_32_b</th>\n",
       "      <th>category_33_b</th>\n",
       "      <th>category_34_b</th>\n",
       "      <th>label_b</th>\n",
       "      <th>title_similarity</th>\n",
       "      <th>common_venue</th>\n",
       "      <th>venue_a_count</th>\n",
       "      <th>venue_b_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43933</td>\n",
       "      <td>535178</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494732</td>\n",
       "      <td>281876</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>381063</td>\n",
       "      <td>47297</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>413037</td>\n",
       "      <td>210346</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44872</td>\n",
       "      <td>76716</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42067</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  paper_a paper_b  year  category_0  category_1  category_2  category_3  \\\n",
       "0   43933  535178  2008           0           0           0           0   \n",
       "1  494732  281876  2004           0           0           0           0   \n",
       "2  381063   47297  2000           1           0           0           0   \n",
       "3  413037  210346  2008           0           0           0           0   \n",
       "4   44872   76716  2007           0           0           0           0   \n",
       "\n",
       "   category_4  category_5  category_6  ...  category_30_b  category_31_b  \\\n",
       "0           0           0           0  ...              0              0   \n",
       "1           0           0           0  ...              0              0   \n",
       "2           0           0           0  ...              0              0   \n",
       "3           0           0           0  ...              0              0   \n",
       "4           0           0           1  ...              0              0   \n",
       "\n",
       "   category_32_b  category_33_b  category_34_b  label_b  title_similarity  \\\n",
       "0              0              0              0        0           0.00000   \n",
       "1              0              0              0        0           0.00000   \n",
       "2              0              0              0        0           0.00000   \n",
       "3              0              0              1        0           0.00000   \n",
       "4              0              0              0        0           0.42067   \n",
       "\n",
       "   common_venue  venue_a_count  venue_b_count  \n",
       "0             1              0              0  \n",
       "1             1              0              0  \n",
       "2             1              0              0  \n",
       "3             1              0              0  \n",
       "4             1              0              0  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlapping papers between train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any overlapping papers between train and test sets\n",
    "train_papers_a = set(X_train['paper_a']).union(set(X_train['paper_b']))\n",
    "test_papers_a = set(X_test['paper_a']).union(set(X_test['paper_b']))\n",
    "\n",
    "# Find intersection between train and test sets to detect any overlapping papers\n",
    "overlap_train_test = train_papers_a.intersection(test_papers_a)\n",
    "\n",
    "if overlap_train_test:\n",
    "    print(\"Overlapping papers found between train and test sets:\", len(overlap_train_test))\n",
    "else:\n",
    "    print(\"No overlapping papers between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.drop(columns=['paper_a', 'paper_b']), X_test.drop(columns=['paper_a', 'paper_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the label to traiun_features and call the var data_train\n",
    "data_train_initial = train_features.copy()\n",
    "data_train_initial['label'] = y_train\n",
    "data_test_initial = test_features.copy()\n",
    "data_test_initial['label'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  paper_a paper_b  year  category_0  category_1  category_2  category_3  \\\n",
      "0   43933  535178  2008           0           0           0           0   \n",
      "1  494732  281876  2004           0           0           0           0   \n",
      "2  381063   47297  2000           1           0           0           0   \n",
      "3  413037  210346  2008           0           0           0           0   \n",
      "4   44872   76716  2007           0           0           0           0   \n",
      "\n",
      "   category_4  category_5  category_6  ...  clustering_coefficient_a  \\\n",
      "0           0           0           0  ...                  0.027778   \n",
      "1           0           0           0  ...                  0.000000   \n",
      "2           0           0           0  ...                  0.000000   \n",
      "3           0           0           0  ...                  0.000000   \n",
      "4           0           0           1  ...                  0.066667   \n",
      "\n",
      "   clustering_coefficient_b  in_degree_a  out_degree_a  in_degree_b  \\\n",
      "0                       0.0          NaN           NaN          NaN   \n",
      "1                       0.0          NaN           NaN          NaN   \n",
      "2                       0.0          NaN           NaN          NaN   \n",
      "3                       0.0          NaN           NaN          NaN   \n",
      "4                       0.0          NaN           NaN          NaN   \n",
      "\n",
      "   out_degree_b  pagerank_a  pagerank_b  eigenvector_centrality_a  \\\n",
      "0           NaN    0.000055    0.000018              6.074048e-05   \n",
      "1           NaN    0.000003    0.000003              4.207958e-27   \n",
      "2           NaN    0.000003    0.000003              2.721114e-24   \n",
      "3           NaN    0.000003    0.000003              3.223694e-25   \n",
      "4           NaN    0.000048    0.000009              6.856272e-06   \n",
      "\n",
      "   eigenvector_centrality_b  \n",
      "0              1.156455e-05  \n",
      "1              2.975476e-27  \n",
      "2              1.408708e-24  \n",
      "3              1.861200e-25  \n",
      "4              6.427559e-07  \n",
      "\n",
      "[5 rows x 97 columns]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def create_graph_from_data(data_train):\n",
    "    # Create an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes and edges from the data\n",
    "    for idx, row in data_train.iterrows():\n",
    "        paper_a = row['paper_a']\n",
    "        paper_b = row['paper_b']\n",
    "        label = row['label']\n",
    "\n",
    "        # Add nodes (papers) and edges (label)\n",
    "        G.add_edge(paper_a, paper_b, weight=label)\n",
    "\n",
    "    return G\n",
    "\n",
    "def calculate_graph_metrics(G):\n",
    "    # Calculate graph metrics\n",
    "    metrics = {}\n",
    "\n",
    "    # Degree centrality\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    metrics['degree_centrality'] = degree_centrality\n",
    "\n",
    "    # Betweenness centrality\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    metrics['betweenness_centrality'] = betweenness_centrality\n",
    "\n",
    "    # Closeness centrality\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    metrics['closeness_centrality'] = closeness_centrality\n",
    "\n",
    "    # Clustering coefficient\n",
    "    clustering_coefficient = nx.clustering(G)\n",
    "    metrics['clustering_coefficient'] = clustering_coefficient\n",
    "\n",
    "    # In-degree and Out-degree (for directed graphs, if applicable)\n",
    "    in_degree = G.in_degree() if G.is_directed() else {}\n",
    "    out_degree = G.out_degree() if G.is_directed() else {}\n",
    "    metrics['in_degree'] = in_degree\n",
    "    metrics['out_degree'] = out_degree\n",
    "\n",
    "    # PageRank\n",
    "    pagerank = nx.pagerank(G)\n",
    "    metrics['pagerank'] = pagerank\n",
    "\n",
    "    # Eigenvector centrality\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "    metrics['eigenvector_centrality'] = eigenvector_centrality\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def add_graph_metrics_to_train(data_train, G):\n",
    "    # Calculate graph metrics\n",
    "    metrics = calculate_graph_metrics(G)\n",
    "\n",
    "    # Add metrics as features to data_train\n",
    "    data_train['degree_centrality_a'] = data_train['paper_a'].map(metrics['degree_centrality'])\n",
    "    data_train['degree_centrality_b'] = data_train['paper_b'].map(metrics['degree_centrality'])\n",
    "    \n",
    "    data_train['betweenness_centrality_a'] = data_train['paper_a'].map(metrics['betweenness_centrality'])\n",
    "    data_train['betweenness_centrality_b'] = data_train['paper_b'].map(metrics['betweenness_centrality'])\n",
    "    \n",
    "    data_train['closeness_centrality_a'] = data_train['paper_a'].map(metrics['closeness_centrality'])\n",
    "    data_train['closeness_centrality_b'] = data_train['paper_b'].map(metrics['closeness_centrality'])\n",
    "    \n",
    "    data_train['clustering_coefficient_a'] = data_train['paper_a'].map(metrics['clustering_coefficient'])\n",
    "    data_train['clustering_coefficient_b'] = data_train['paper_b'].map(metrics['clustering_coefficient'])\n",
    "\n",
    "    # Add in-degree and out-degree metrics\n",
    "    data_train['in_degree_a'] = data_train['paper_a'].map(metrics['in_degree'])\n",
    "    data_train['out_degree_a'] = data_train['paper_a'].map(metrics['out_degree'])\n",
    "    data_train['in_degree_b'] = data_train['paper_b'].map(metrics['in_degree'])\n",
    "    data_train['out_degree_b'] = data_train['paper_b'].map(metrics['out_degree'])\n",
    "\n",
    "    # Add PageRank and Eigenvector centrality metrics\n",
    "    data_train['pagerank_a'] = data_train['paper_a'].map(metrics['pagerank'])\n",
    "    data_train['pagerank_b'] = data_train['paper_b'].map(metrics['pagerank'])\n",
    "    \n",
    "    data_train['eigenvector_centrality_a'] = data_train['paper_a'].map(metrics['eigenvector_centrality'])\n",
    "    data_train['eigenvector_centrality_b'] = data_train['paper_b'].map(metrics['eigenvector_centrality'])\n",
    "\n",
    "    return data_train\n",
    "\n",
    "G_train = create_graph_from_data(data_train_initial)  # Create the graph from the training data\n",
    "G_test = create_graph_from_data(data_test_initial)  # Create the graph from the testing data\n",
    "data_train_with_metrics = add_graph_metrics_to_train(data_train_initial, G_train)  # Add graph metrics to train data\n",
    "data_test_with_metrics = add_graph_metrics_to_train(data_test_initial, G_test)  # Add graph\n",
    "\n",
    "# Print the updated data_train with graph metrics\n",
    "print(data_train_with_metrics.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_with_metrics.to_pickle('Data_metrics\\\\data_train_with_metrics.pkl')\n",
    "data_test_with_metrics.to_pickle('Data_metrics\\\\data_test_with_metrics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_pickle('Data_metrics\\\\data_train_with_metrics.pkl')\n",
    "data_test = pd.read_pickle('Data_metrics\\\\data_test_with_metrics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_a</th>\n",
       "      <th>paper_b</th>\n",
       "      <th>year</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>category_6</th>\n",
       "      <th>...</th>\n",
       "      <th>clustering_coefficient_a</th>\n",
       "      <th>clustering_coefficient_b</th>\n",
       "      <th>in_degree_a</th>\n",
       "      <th>out_degree_a</th>\n",
       "      <th>in_degree_b</th>\n",
       "      <th>out_degree_b</th>\n",
       "      <th>pagerank_a</th>\n",
       "      <th>pagerank_b</th>\n",
       "      <th>eigenvector_centrality_a</th>\n",
       "      <th>eigenvector_centrality_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43933</td>\n",
       "      <td>535178</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>6.074048e-05</td>\n",
       "      <td>1.156455e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494732</td>\n",
       "      <td>281876</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.207958e-27</td>\n",
       "      <td>2.975476e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>381063</td>\n",
       "      <td>47297</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.721114e-24</td>\n",
       "      <td>1.408708e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>413037</td>\n",
       "      <td>210346</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.223694e-25</td>\n",
       "      <td>1.861200e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44872</td>\n",
       "      <td>76716</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>6.856272e-06</td>\n",
       "      <td>6.427559e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>556462</td>\n",
       "      <td>603428</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>2.781278e-04</td>\n",
       "      <td>8.142519e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>535583</td>\n",
       "      <td>477302</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>4.840745e-05</td>\n",
       "      <td>3.721918e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>353302</td>\n",
       "      <td>94400</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>2.992129e-05</td>\n",
       "      <td>2.254561e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>481166</td>\n",
       "      <td>330625</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.207958e-27</td>\n",
       "      <td>2.975476e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>345593</td>\n",
       "      <td>214586</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.207958e-27</td>\n",
       "      <td>2.975476e-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper_a paper_b  year  category_0  category_1  category_2  category_3  \\\n",
       "0       43933  535178  2008           0           0           0           0   \n",
       "1      494732  281876  2004           0           0           0           0   \n",
       "2      381063   47297  2000           1           0           0           0   \n",
       "3      413037  210346  2008           0           0           0           0   \n",
       "4       44872   76716  2007           0           0           0           0   \n",
       "...       ...     ...   ...         ...         ...         ...         ...   \n",
       "99995  556462  603428  1999           0           0           0           0   \n",
       "99996  535583  477302  1991           0           0           0           0   \n",
       "99997  353302   94400  2008           0           0           0           0   \n",
       "99998  481166  330625  1990           0           0           0           0   \n",
       "99999  345593  214586  2007           0           0           0           0   \n",
       "\n",
       "       category_4  category_5  category_6  ...  clustering_coefficient_a  \\\n",
       "0               0           0           0  ...                  0.027778   \n",
       "1               0           0           0  ...                  0.000000   \n",
       "2               0           0           0  ...                  0.000000   \n",
       "3               0           0           0  ...                  0.000000   \n",
       "4               0           0           1  ...                  0.066667   \n",
       "...           ...         ...         ...  ...                       ...   \n",
       "99995           0           0           0  ...                  0.051282   \n",
       "99996           0           0           0  ...                  0.047619   \n",
       "99997           0           0           0  ...                  0.000000   \n",
       "99998           0           0           0  ...                  0.000000   \n",
       "99999           0           0           0  ...                  0.000000   \n",
       "\n",
       "       clustering_coefficient_b  in_degree_a  out_degree_a  in_degree_b  \\\n",
       "0                      0.000000          NaN           NaN          NaN   \n",
       "1                      0.000000          NaN           NaN          NaN   \n",
       "2                      0.000000          NaN           NaN          NaN   \n",
       "3                      0.000000          NaN           NaN          NaN   \n",
       "4                      0.000000          NaN           NaN          NaN   \n",
       "...                         ...          ...           ...          ...   \n",
       "99995                  0.000000          NaN           NaN          NaN   \n",
       "99996                  0.333333          NaN           NaN          NaN   \n",
       "99997                  0.166667          NaN           NaN          NaN   \n",
       "99998                  0.000000          NaN           NaN          NaN   \n",
       "99999                  0.000000          NaN           NaN          NaN   \n",
       "\n",
       "       out_degree_b  pagerank_a  pagerank_b  eigenvector_centrality_a  \\\n",
       "0               NaN    0.000055    0.000018              6.074048e-05   \n",
       "1               NaN    0.000003    0.000003              4.207958e-27   \n",
       "2               NaN    0.000003    0.000003              2.721114e-24   \n",
       "3               NaN    0.000003    0.000003              3.223694e-25   \n",
       "4               NaN    0.000048    0.000009              6.856272e-06   \n",
       "...             ...         ...         ...                       ...   \n",
       "99995           NaN    0.000068    0.000011              2.781278e-04   \n",
       "99996           NaN    0.000056    0.000019              4.840745e-05   \n",
       "99997           NaN    0.000030    0.000023              2.992129e-05   \n",
       "99998           NaN    0.000003    0.000003              4.207958e-27   \n",
       "99999           NaN    0.000003    0.000003              4.207958e-27   \n",
       "\n",
       "       eigenvector_centrality_b  \n",
       "0                  1.156455e-05  \n",
       "1                  2.975476e-27  \n",
       "2                  1.408708e-24  \n",
       "3                  1.861200e-25  \n",
       "4                  6.427559e-07  \n",
       "...                         ...  \n",
       "99995              8.142519e-05  \n",
       "99996              3.721918e-04  \n",
       "99997              2.254561e-04  \n",
       "99998              2.975476e-27  \n",
       "99999              2.975476e-27  \n",
       "\n",
       "[100000 rows x 97 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[data_train['in_degree_a'].isnull() & data_train['in_degree_b'].isnull() & data_train['out_degree_a'].isnull() & data_train['out_degree_b'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with more than 5000 nan\n",
    "data_train = data_train.dropna(axis=1, thresh=5000)\n",
    "data_test = data_test.dropna(axis=1, thresh=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_a</th>\n",
       "      <th>paper_b</th>\n",
       "      <th>year</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>category_6</th>\n",
       "      <th>...</th>\n",
       "      <th>betweenness_centrality_a</th>\n",
       "      <th>betweenness_centrality_b</th>\n",
       "      <th>closeness_centrality_a</th>\n",
       "      <th>closeness_centrality_b</th>\n",
       "      <th>clustering_coefficient_a</th>\n",
       "      <th>clustering_coefficient_b</th>\n",
       "      <th>pagerank_a</th>\n",
       "      <th>pagerank_b</th>\n",
       "      <th>eigenvector_centrality_a</th>\n",
       "      <th>eigenvector_centrality_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43933</td>\n",
       "      <td>535178</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.928796e-04</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.051122</td>\n",
       "      <td>0.048010</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>6.074048e-05</td>\n",
       "      <td>1.156455e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494732</td>\n",
       "      <td>281876</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.842711e-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.207958e-27</td>\n",
       "      <td>2.975476e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>381063</td>\n",
       "      <td>47297</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.213553e-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.721114e-24</td>\n",
       "      <td>1.408708e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>413037</td>\n",
       "      <td>210346</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.528132e-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.223694e-25</td>\n",
       "      <td>1.861200e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44872</td>\n",
       "      <td>76716</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.138653e-05</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.046427</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>6.856272e-06</td>\n",
       "      <td>6.427559e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  paper_a paper_b  year  category_0  category_1  category_2  category_3  \\\n",
       "0   43933  535178  2008           0           0           0           0   \n",
       "1  494732  281876  2004           0           0           0           0   \n",
       "2  381063   47297  2000           1           0           0           0   \n",
       "3  413037  210346  2008           0           0           0           0   \n",
       "4   44872   76716  2007           0           0           0           0   \n",
       "\n",
       "   category_4  category_5  category_6  ...  betweenness_centrality_a  \\\n",
       "0           0           0           0  ...              1.928796e-04   \n",
       "1           0           0           0  ...              1.842711e-10   \n",
       "2           0           0           0  ...              9.213553e-10   \n",
       "3           0           0           0  ...              5.528132e-10   \n",
       "4           0           0           1  ...              7.138653e-05   \n",
       "\n",
       "   betweenness_centrality_b  closeness_centrality_a  closeness_centrality_b  \\\n",
       "0                  0.000083                0.051122                0.048010   \n",
       "1                  0.000000                0.000019                0.000013   \n",
       "2                  0.000000                0.000023                0.000017   \n",
       "3                  0.000000                0.000022                0.000015   \n",
       "4                  0.000017                0.046427                0.041905   \n",
       "\n",
       "   clustering_coefficient_a  clustering_coefficient_b  pagerank_a  pagerank_b  \\\n",
       "0                  0.027778                       0.0    0.000055    0.000018   \n",
       "1                  0.000000                       0.0    0.000003    0.000003   \n",
       "2                  0.000000                       0.0    0.000003    0.000003   \n",
       "3                  0.000000                       0.0    0.000003    0.000003   \n",
       "4                  0.066667                       0.0    0.000048    0.000009   \n",
       "\n",
       "   eigenvector_centrality_a  eigenvector_centrality_b  \n",
       "0              6.074048e-05              1.156455e-05  \n",
       "1              4.207958e-27              2.975476e-27  \n",
       "2              2.721114e-24              1.408708e-24  \n",
       "3              3.223694e-25              1.861200e-25  \n",
       "4              6.856272e-06              6.427559e-07  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_a</th>\n",
       "      <th>paper_b</th>\n",
       "      <th>year</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>category_6</th>\n",
       "      <th>...</th>\n",
       "      <th>betweenness_centrality_a</th>\n",
       "      <th>betweenness_centrality_b</th>\n",
       "      <th>closeness_centrality_a</th>\n",
       "      <th>closeness_centrality_b</th>\n",
       "      <th>clustering_coefficient_a</th>\n",
       "      <th>clustering_coefficient_b</th>\n",
       "      <th>pagerank_a</th>\n",
       "      <th>pagerank_b</th>\n",
       "      <th>eigenvector_centrality_a</th>\n",
       "      <th>eigenvector_centrality_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29293</td>\n",
       "      <td>237017</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.058936e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>1.777466e-21</td>\n",
       "      <td>1.026221e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>495464</td>\n",
       "      <td>53963</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.529786e-09</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>1.010360e-23</td>\n",
       "      <td>1.428864e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487473</td>\n",
       "      <td>154225</td>\n",
       "      <td>1989</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.499441e-05</td>\n",
       "      <td>2.051617e-04</td>\n",
       "      <td>0.025925</td>\n",
       "      <td>0.028398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>4.473533e-06</td>\n",
       "      <td>2.997120e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>408586</td>\n",
       "      <td>164311</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.529786e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.428864e-23</td>\n",
       "      <td>1.010360e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220075</td>\n",
       "      <td>420821</td>\n",
       "      <td>1993</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.058936e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.401843e-21</td>\n",
       "      <td>8.093546e-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  paper_a paper_b  year  category_0  category_1  category_2  category_3  \\\n",
       "0   29293  237017  2006           0           0           0           0   \n",
       "1  495464   53963  2009           0           0           0           0   \n",
       "2  487473  154225  1989           0           1           0           0   \n",
       "3  408586  164311  2008           0           0           0           0   \n",
       "4  220075  420821  1993           0           0           0           0   \n",
       "\n",
       "   category_4  category_5  category_6  ...  betweenness_centrality_a  \\\n",
       "0           0           0           0  ...              1.058936e-08   \n",
       "1           0           0           0  ...              0.000000e+00   \n",
       "2           0           1           0  ...              2.499441e-05   \n",
       "3           0           0           0  ...              3.529786e-09   \n",
       "4           0           0           0  ...              1.058936e-08   \n",
       "\n",
       "   betweenness_centrality_b  closeness_centrality_a  closeness_centrality_b  \\\n",
       "0              0.000000e+00                0.000126                0.000076   \n",
       "1              3.529786e-09                0.000056                0.000084   \n",
       "2              2.051617e-04                0.025925                0.028398   \n",
       "3              0.000000e+00                0.000084                0.000056   \n",
       "4              0.000000e+00                0.000096                0.000067   \n",
       "\n",
       "   clustering_coefficient_a  clustering_coefficient_b  pagerank_a  pagerank_b  \\\n",
       "0                       0.0                       0.0    0.000078    0.000078   \n",
       "1                       0.0                       0.0    0.000061    0.000110   \n",
       "2                       0.0                       0.0    0.000078    0.000194   \n",
       "3                       0.0                       0.0    0.000012    0.000012   \n",
       "4                       0.0                       0.0    0.000012    0.000012   \n",
       "\n",
       "   eigenvector_centrality_a  eigenvector_centrality_b  \n",
       "0              1.777466e-21              1.026221e-21  \n",
       "1              1.010360e-23              1.428864e-23  \n",
       "2              4.473533e-06              2.997120e-05  \n",
       "3              1.428864e-23              1.010360e-23  \n",
       "4              1.401843e-21              8.093546e-22  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a Graph Convolutional Network (GCN) that leverages graph-structured data to learn node representations through message-passing, using two graph convolution layers followed by a sigmoid output for binary classification. Despite its ability to capture relationships between nodes, the model performs suboptimally with an accuracy around 0.7305, indicating room for improvement in both architecture and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = data_train.drop(columns=['label', 'paper_a', 'paper_b']), data_train['label'], data_test.drop(columns=['label','paper_a', 'paper_b']), data_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Step 1: Define the function to create edge indices\n",
    "def create_edge_index(data, num_nodes):\n",
    "    # Create a mapping from unique paper IDs to numeric IDs\n",
    "    paper_ids = pd.concat([data['paper_a'], data['paper_b']]).unique()\n",
    "    paper_id_map = {paper: idx for idx, paper in enumerate(paper_ids)}\n",
    "\n",
    "    edge_index = []\n",
    "    for _, row in data.iterrows():\n",
    "        if row['label'] == 1:  # Add an edge if there's a citation\n",
    "            paper_a_id = paper_id_map.get(row['paper_a'])\n",
    "            paper_b_id = paper_id_map.get(row['paper_b'])\n",
    "            if paper_a_id is not None and paper_b_id is not None:\n",
    "                # Ensure indices are within bounds\n",
    "                if paper_a_id < num_nodes and paper_b_id < num_nodes:\n",
    "                    edge_index.append([paper_a_id, paper_b_id])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Validate that all indices are within bounds\n",
    "    if edge_index.numel() > 0 and edge_index.max().item() >= num_nodes:\n",
    "        raise ValueError(\"Edge index contains out-of-bound indices!\")\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare edge indices\n",
    "num_nodes_train = X_train.shape[0]\n",
    "num_nodes_test = X_test.shape[0]\n",
    "\n",
    "edge_index_train = create_edge_index(data_train_initial, num_nodes=num_nodes_train)\n",
    "edge_index_test = create_edge_index(data_test_initial, num_nodes=num_nodes_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[100000, 90], edge_index=[2, 48693], y=[100000])\n",
      "Data(x=[20000, 90], edge_index=[2, 8678], y=[20000])\n"
     ]
    }
   ],
   "source": [
    "# i want hewre to create a graph for the data_train without the label and add the label as a edge in the graph, then i want to calculate the metrics for the graph and add 9it to the train data \n",
    "# Step 3: Create PyG Data objects\n",
    "data_train = Data(x=torch.tensor(X_train.values, dtype=torch.float),\n",
    "                  edge_index=edge_index_train, y=torch.tensor(y_train.values, dtype=torch.float),\n",
    "                  pos_edge_index=None, neg_edge_index=None).to(device)\n",
    "\n",
    "data_test = Data(x=torch.tensor(X_test.values, dtype=torch.float),\n",
    "                    edge_index=edge_index_test, y=torch.tensor(y_test.values, dtype=torch.float),\n",
    "                    pos_edge_index=None, neg_edge_index=None).to(device)\n",
    "\n",
    "print(data_train)\n",
    "print(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 1)  # Binary output\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.sigmoid(x).squeeze()  # Binary probability output\n",
    "\n",
    "# Step 5: Train the GCN Model\n",
    "def train_model(model, data, epochs=1000, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "# Step 6: Initialize and train GCN\n",
    "gcn = GCN(num_node_features=X_train.shape[1])\n",
    "gcn = gcn.to(device)\n",
    "#gcn = train_model(gcn, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GCN:\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate GCN Model\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        predictions = (out > 0.5).float()  # Convert probabilities to binary predictions\n",
    "        accuracy = (predictions == data.y).sum().item() / len(data.y)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        return predictions\n",
    "\n",
    "print(\"\\nEvaluating GCN:\")\n",
    "#gcn_predictions = evaluate_model(gcn, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## improved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying model with an additional convolutional layer, applies dropout after each convolutional layer to prevent overfitting, and incorporates early stopping during training to halt the process when the loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ImprovedGCN(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, hidden_units=64, dropout_rate=0.5):\n",
    "        super(ImprovedGCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(num_node_features, hidden_units)\n",
    "        self.conv2 = GCNConv(hidden_units, hidden_units)\n",
    "        self.conv3 = GCNConv(hidden_units, num_classes)  # Third layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        return torch.sigmoid(x)  # Use sigmoid for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, epochs=100, lr=0.01, patience=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data).squeeze()  # Squeeze to match the shape of the labels\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter > patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data).squeeze()  # Squeeze to match the labels shape\n",
    "        pred = (out > 0.5).float()  # Convert to binary predictions (0 or 1)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = (pred == data.y).sum().item() / len(data.y)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return out, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Improved GCN\n",
    "gcn = ImprovedGCN(num_node_features=X_train.shape[1], num_classes=1)\n",
    "gcn = gcn.to(device)\n",
    "# gcn = train_model(gcn, data_train, epochs=1000, lr=0.005, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GCN:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating GCN:\")\n",
    "#gcn_predictions = evaluate_model(gcn, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really improved\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying new method\n",
    "See if improves if the model is more compplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('Data_metrics\\\\data_train_with_metrics.pkl')\n",
    "test_data = pd.read_pickle('Data_metrics\\\\data_test_with_metrics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_data.drop(columns=['label_a', 'label_b']), test_data.drop(columns=['label_a', 'label_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(axis=1, thresh=5000)\n",
    "test_data = test_data.dropna(axis=1, thresh=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_a', 'paper_b', 'year', 'category_0', 'category_1', 'category_2',\n",
       "       'category_3', 'category_4', 'category_5', 'category_6', 'category_7',\n",
       "       'category_8', 'category_9', 'category_10', 'category_11', 'category_12',\n",
       "       'category_13', 'category_14', 'category_15', 'category_16',\n",
       "       'category_17', 'category_18', 'category_19', 'category_20',\n",
       "       'category_21', 'category_22', 'category_23', 'category_24',\n",
       "       'category_25', 'category_26', 'category_27', 'category_28',\n",
       "       'category_29', 'category_30', 'category_31', 'category_32',\n",
       "       'category_33', 'category_34', 'year_b', 'category_0_b', 'category_1_b',\n",
       "       'category_2_b', 'category_3_b', 'category_4_b', 'category_5_b',\n",
       "       'category_6_b', 'category_7_b', 'category_8_b', 'category_9_b',\n",
       "       'category_10_b', 'category_11_b', 'category_12_b', 'category_13_b',\n",
       "       'category_14_b', 'category_15_b', 'category_16_b', 'category_17_b',\n",
       "       'category_18_b', 'category_19_b', 'category_20_b', 'category_21_b',\n",
       "       'category_22_b', 'category_23_b', 'category_24_b', 'category_25_b',\n",
       "       'category_26_b', 'category_27_b', 'category_28_b', 'category_29_b',\n",
       "       'category_30_b', 'category_31_b', 'category_32_b', 'category_33_b',\n",
       "       'category_34_b', 'title_similarity', 'common_venue', 'venue_a_count',\n",
       "       'venue_b_count', 'label', 'degree_centrality_a', 'degree_centrality_b',\n",
       "       'betweenness_centrality_a', 'betweenness_centrality_b',\n",
       "       'closeness_centrality_a', 'closeness_centrality_b',\n",
       "       'clustering_coefficient_a', 'clustering_coefficient_b', 'pagerank_a',\n",
       "       'pagerank_b', 'eigenvector_centrality_a', 'eigenvector_centrality_b'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Step 2: Preprocess Features\n",
    "def preprocess_data(df):\n",
    "    features = []\n",
    "    for col in df.columns:\n",
    "        if col not in ['paper_a', 'paper_b', 'label']:\n",
    "            features.append(df[col].values)\n",
    "    features = np.column_stack(features)\n",
    "    return features\n",
    "\n",
    "X_train = preprocess_data(train_data)\n",
    "X_test = preprocess_data(test_data)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Create Edge Indices\n",
    "def create_edge_index(data, num_nodes):\n",
    "    paper_ids = pd.concat([data['paper_a'], data['paper_b']]).unique()\n",
    "    paper_id_map = {paper: idx for idx, paper in enumerate(paper_ids)}\n",
    "\n",
    "    edge_index = []\n",
    "    for _, row in data.iterrows():\n",
    "        if row['label'] == 1:  # Add an edge for citation\n",
    "            paper_a_id = paper_id_map.get(row['paper_a'])\n",
    "            paper_b_id = paper_id_map.get(row['paper_b'])\n",
    "            if paper_a_id is not None and paper_b_id is not None:\n",
    "                if paper_a_id < num_nodes and paper_b_id < num_nodes:\n",
    "                    edge_index.append([paper_a_id, paper_b_id])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Validate edge indices\n",
    "    if edge_index.numel() > 0 and edge_index.max().item() >= num_nodes:\n",
    "        raise ValueError(\"Edge index contains out-of-bound indices!\")\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "num_nodes_train = X_train.shape[0]\n",
    "num_nodes_test = X_test.shape[0]\n",
    "\n",
    "edge_index_train = create_edge_index(train_data, num_nodes=num_nodes_train)\n",
    "edge_index_test = create_edge_index(test_data, num_nodes=num_nodes_test)\n",
    "\n",
    "# Step 4: Create PyTorch Geometric Data Objects\n",
    "data_train = Data(\n",
    "    x=torch.tensor(X_train, dtype=torch.float),\n",
    "    edge_index=edge_index_train,\n",
    "    y=torch.tensor(y_train.values, dtype=torch.float)\n",
    ")\n",
    "\n",
    "data_test = Data(\n",
    "    x=torch.tensor(X_test, dtype=torch.float),\n",
    "    edge_index=edge_index_test,\n",
    "    y=torch.tensor(y_test.values, dtype=torch.float)\n",
    ")\n",
    "data_train = data_train.to(device)\n",
    "data_test = data_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7626\n",
      "Epoch 10, Loss: 0.6629\n",
      "Epoch 20, Loss: 0.5370\n",
      "Epoch 30, Loss: 0.4676\n",
      "Epoch 40, Loss: 0.4245\n",
      "Epoch 50, Loss: 0.3854\n",
      "Epoch 60, Loss: 0.3572\n",
      "Epoch 70, Loss: 0.3431\n",
      "Epoch 80, Loss: 0.3316\n",
      "Epoch 90, Loss: 0.3233\n",
      "Epoch 100, Loss: 0.3157\n",
      "Epoch 110, Loss: 0.3135\n",
      "Epoch 120, Loss: 0.3072\n",
      "Epoch 130, Loss: 0.3082\n",
      "Early stopping at epoch 139\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Step 1: Define a more complex GCN model with dense layers added\n",
    "class ComplexGCNWithDense(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, hidden_units=128, dropout_rate=0.5):\n",
    "        super(ComplexGCNWithDense, self).__init__()\n",
    "\n",
    "        # Layer 1: First Convolutional Layer\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_units)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_units)  # Batch Normalization\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 2: Second Convolutional Layer\n",
    "        self.conv2 = GCNConv(hidden_units, hidden_units * 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_units * 2)  # Batch Normalization\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 3: Third Convolutional Layer\n",
    "        self.conv3 = GCNConv(hidden_units * 2, hidden_units * 2)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_units * 2)  # Batch Normalization\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 4: Fourth Convolutional Layer\n",
    "        self.conv4 = GCNConv(hidden_units * 2, hidden_units * 2)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_units * 2)  # Batch Normalization\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 5: Final Convolutional Layer (to reduce to single output node)\n",
    "        self.conv5 = GCNConv(hidden_units * 2, num_classes)\n",
    "\n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(num_classes, hidden_units * 2)  # Dense layer after GCN layers\n",
    "        self.bn_fc1 = nn.BatchNorm1d(hidden_units * 2)  # Batch Normalization for FC layer\n",
    "        self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_units * 2, hidden_units)  # Dense layer\n",
    "        self.bn_fc2 = nn.BatchNorm1d(hidden_units)  # Batch Normalization for FC layer\n",
    "        self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_units, num_classes)  # Output layer for binary classification\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First Layer (GCN + Dropout + BatchNorm)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second Layer (GCN + Dropout + BatchNorm)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Third Layer (GCN + Dropout + BatchNorm)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Fourth Layer (GCN + Dropout + BatchNorm)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Fifth Layer: Output (GCN)\n",
    "        x = self.conv5(x, edge_index)\n",
    "        \n",
    "        # Flatten and pass through fully connected (dense) layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.dropout_fc1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn_fc2(x)\n",
    "        x = self.dropout_fc2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x).squeeze()  # Binary output for classification\n",
    "\n",
    "# Step 2: Train the Model (same as before)\n",
    "def train_model(model, data, epochs=1000, lr=0.01, patience=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data).squeeze()  # Squeeze to match the shape of the labels\n",
    "        loss = F.binary_cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter > patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Step 3: Initialize and Train the Model\n",
    "gcn = ComplexGCNWithDense(num_node_features=X_train.shape[1], num_classes=1, hidden_units=128, dropout_rate=0.5)\n",
    "gcn = gcn.to(device)\n",
    "gcn = train_model(gcn, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8550\n",
      "Test Precision: 0.8425\n",
      "Test Recall: 0.8733\n",
      "Test F1 Score: 0.8576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "def evaluate_model(model, data, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    data = data.to(device)  # Move the data to the appropriate device (GPU or CPU)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to get predictions\n",
    "        out = model(data)\n",
    "        predictions = (out >= 0.5).float()  # Apply threshold\n",
    "\n",
    "        # Convert tensors to lists for sklearn metrics\n",
    "        y_true = data.y.detach().cpu().tolist()\n",
    "        y_pred = predictions.detach().cpu().tolist()\n",
    "\n",
    "        # Compute metrics using sklearn\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Ensure test data is on the correct device\n",
    "data_test = data_test.to(device)\n",
    "\n",
    "# Perform evaluation on test data\n",
    "accuracy, precision, recall, f1 = evaluate_model(gcn, data_test, device)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained seems to not be better than the normal models, probably these models need to be tuned better and they may be too simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
